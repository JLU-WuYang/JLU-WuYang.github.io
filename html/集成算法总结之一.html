<html>
	<head>
	<meta charset="UTF-8">
	<title>集成算法总结之一</title>
	<link type="text/css" rel="stylesheet" href="../naver.css">
	<link type="text/css" rel="stylesheet" href="../css/article.css">
		<link rel="icon" href="../image/logo.png">
	</head>
	
    <body>
	<div id="naver">
		<img src="../image/logo.png" height="70px">
		<ul id="naver_header">
		<li class="top" ><a href="../index.html" class="top1" title="HOME"><img src="../image/home.png"></a></li>
		<li class="top"><a href="blog.html" class="top1"title="BLOG"><img src="../image/blog.png"></a></li>
		<li class="top"><a href="aboutme.html" class="top1"title="ABOUT ME"><img src="../image/aboutme.png"></a></li>		

		</ul>	
	</div>
<div id="main" class="blog">
<br><br>
<h1>集成算法总结之一</h1>
<p>  <strong>Ensemble strategies/solution</strong>,即集成方法。前一段时间，看了两篇英文论文，一篇讨论了神经网络的稳定性问题，
另一篇讨论关于经济决策，它们都采用了集成方法以改善预测结果。
集成算法有Boosting，Bagging，Stacking..</p>
<blockquote>
Boosting算法  
Boosting is also a re-sampling strategy,with a probability distribution that is dependent on the misclassification rate for each observation.
Boosting is an iterative algorithm where the probility of the misclassified observations is increased and the corresponding probility of correctly
classified observations is decreased over time. As boosting progresses ,the composition of the training sets becomes increasingly dominated by 
hard-to-classify examples.   
</blockquote>
<p>
  Boosting算法是一种策略，其实可以说成一个族，我们通常提到的是AdaBoost算法。   
给定一弱学习算法（学习算法识别一组概念仅比随机猜测好）和训练集(X1,Y1),(X2,Y2)...(Xn,Yn), <strong>Y</strong>对于分类问题是类别标志对于回归问题是数值。    
初始化对每一个训练例赋相等权重1/n，然后用该训练算法训练一轮，训练后对于失败的例子赋予较大权重，即让学习算法在后续的学习过程中  
着重学习，另外还会产生一个预测函数序列h1,h2,..ht(t为训练轮数)与与之对应的权重w1,w2,...wt。预测效果好的函数权重大，最终结果采取加权平均（预测）或有权重
投票（分类）。</p>  
总结一下：
<ol> 
<li>样例有权重（预测错与对，错的权重大），为让学习算法着重学习以前犯错的样例。（人也是这样，对于错题着重练习，才能提分嘛）。顺便提一下：对于用S型函数神经网络的学习算法，Cost函数一般选取的是交叉熵，为什么不是二次代价函数呢？因为交叉熵在犯错误时学的更快。从Cost-iteratorNum函数图像会发现，斜率的绝对值很大。</li>
<li>函数有权重（预测的好坏，好的权重大），为了让总体预测效果好。</li>
</ol>
让我们看一下伪代码

<div class="code">
    <pre>
    U=(1/n,....,1/n)//样本权重
    for(j=1;j<T;j++){
    hj=P(x,y,U);//生成预测函数
    e=sum(i:hj(xi)!=yi)Ui;
    if(e>1/2){//连一半准确度都没有不符合弱学习
    j=T;
    break;
    }
    wj=ln((1-e)/e);//预测hj的权值 w>0定成立，且对于e为递减函数
    for(i=1;i<=n;i++)
      if(hj(xi)!=yi)
      Ui=Ui/(2e);//e小于1/2故权重增加
      else
      Ui=Ui/(2(1-e));//权重减小
    for(int i=1;i<=n;i++)
      Ui=Ui/Sum(Ui);//使总和为1 The trick is used usually when we deal with probability problem. 
    }  
    </pre>
</div>
<br>
 输出为预测函数列(h1,h2,h3...hT)与对应权值(w1,w2,w3,...wT)
最终预测为H=sign(sum(wi*hi(xi)--i))  
<p>Image:
<img src="http://img.blog.csdn.net/20131103130244828?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvRGFya19TY29wZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="image for understanding this mothod easily">
</p>

References：
<ul>
<li>沈学华周志华, Boosting和Bagging综述</li>  

<li>David West, Neural network ensemble strategies for financial decision applications</li>  

<li>John Carney, Stability problems with artificial neural networks and the ensemble solution</li>  
</ul>
<br><br><br>
</div>

<footer>

&copy;2016, 吴洋<br>
E-mail:albertwyjoy@gmail.com
</footer>
  

</body>

</html>
